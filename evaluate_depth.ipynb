{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from layers import disp_to_depth\n",
    "from utils import readlines\n",
    "from options import MonodepthOptions\n",
    "\n",
    "#MonodepthOptions is important!!\n",
    "\n",
    "\n",
    "import datasets\n",
    "import networks\n",
    "\n",
    "#env:pytorch_p36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.setNumThreads(0)  # This speeds up evaluation 5x on our unix systems (OpenCV 3.3.1)\n",
    "\n",
    "\n",
    "splits_dir = \"./splits\"\n",
    "                          \n",
    "\n",
    "# Models which were trained with stereo supervision were trained with a nominal\n",
    "# baseline of 0.1 units. The KITTI rig has a baseline of 54cm. Therefore,\n",
    "# to convert our stereo predictions to real-world scale we multiply our depths by 5.4.\n",
    "STEREO_SCALE_FACTOR = 5.4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_errors(gt, pred):\n",
    "    \"\"\"Computation of error metrics between predicted and ground truth depths\n",
    "    \"\"\"\n",
    "    thresh = np.maximum((gt / pred), (pred / gt))\n",
    "    a1 = (thresh < 1.25     ).mean()\n",
    "    a2 = (thresh < 1.25 ** 2).mean()\n",
    "    a3 = (thresh < 1.25 ** 3).mean()\n",
    "\n",
    "    rmse = (gt - pred) ** 2\n",
    "    rmse = np.sqrt(rmse.mean())\n",
    "\n",
    "    rmse_log = (np.log(gt) - np.log(pred)) ** 2\n",
    "    rmse_log = np.sqrt(rmse_log.mean())\n",
    "\n",
    "    abs_rel = np.mean(np.abs(gt - pred) / gt)\n",
    "\n",
    "    sq_rel = np.mean(((gt - pred) ** 2) / gt)\n",
    "\n",
    "    return abs_rel, sq_rel, rmse, rmse_log, a1, a2, a3\n",
    "\n",
    "\n",
    "def batch_post_process_disparity(l_disp, r_disp):\n",
    "    \"\"\"Apply the disparity post-processing method as introduced in Monodepthv1\n",
    "    \"\"\"\n",
    "    _, h, w = l_disp.shape\n",
    "    m_disp = 0.5 * (l_disp + r_disp)\n",
    "    l, _ = np.meshgrid(np.linspace(0, 1, w), np.linspace(0, 1, h))\n",
    "    l_mask = (1.0 - np.clip(20 * (l - 0.05), 0, 1))[None, ...]\n",
    "    r_mask = l_mask[:, :, ::-1]\n",
    "    return r_mask * l_disp + l_mask * r_disp + (1.0 - l_mask - r_mask) * m_disp\n",
    "\n",
    "\n",
    "def evaluate(opt):\n",
    "    \"\"\"Evaluates a pretrained model using a specified test set\n",
    "    \"\"\"\n",
    "    MIN_DEPTH = 1e-3\n",
    "    MAX_DEPTH = 80\n",
    "\n",
    "    assert sum((opt.eval_mono, opt.eval_stereo)) == 1, \\\n",
    "        \"Please choose mono or stereo evaluation by setting either --eval_mono or --eval_stereo\"\n",
    "\n",
    "    if opt.ext_disp_to_eval is None:\n",
    "\n",
    "        opt.load_weights_folder = os.path.expanduser(opt.load_weights_folder)\n",
    "\n",
    "        assert os.path.isdir(opt.load_weights_folder), \\\n",
    "            \"Cannot find a folder at {}\".format(opt.load_weights_folder)\n",
    "\n",
    "        print(\"-> Loading weights from {}\".format(opt.load_weights_folder))\n",
    "\n",
    "        filenames = readlines(os.path.join(splits_dir, opt.eval_split, \"test_files.txt\"))\n",
    "        encoder_path = os.path.join(opt.load_weights_folder, \"encoder.pth\")\n",
    "        decoder_path = os.path.join(opt.load_weights_folder, \"depth.pth\")\n",
    "\n",
    "        encoder_dict = torch.load(encoder_path)\n",
    "\n",
    "        dataset = datasets.KITTIRAWDataset(opt.data_path, filenames,\n",
    "                                           encoder_dict['height'], encoder_dict['width'],\n",
    "                                           [0], 4, is_train=False)\n",
    "        dataloader = DataLoader(dataset, 16, shuffle=False, num_workers=opt.num_workers,\n",
    "                                pin_memory=True, drop_last=False)\n",
    "\n",
    "        encoder = networks.ResnetEncoder(opt.num_layers, False)\n",
    "        depth_decoder = networks.DepthDecoder(encoder.num_ch_enc)\n",
    "\n",
    "        model_dict = encoder.state_dict()\n",
    "        encoder.load_state_dict({k: v for k, v in encoder_dict.items() if k in model_dict})\n",
    "        depth_decoder.load_state_dict(torch.load(decoder_path))\n",
    "\n",
    "        encoder.cuda()\n",
    "        encoder.eval()\n",
    "        depth_decoder.cuda()\n",
    "        depth_decoder.eval()\n",
    "\n",
    "        pred_disps = []\n",
    "\n",
    "        print(\"-> Computing predictions with size {}x{}\".format(\n",
    "            encoder_dict['width'], encoder_dict['height']))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data in dataloader:\n",
    "                input_color = data[(\"color\", 0, 0)].cuda()\n",
    "\n",
    "                if opt.post_process:\n",
    "                    # Post-processed results require each image to have two forward passes\n",
    "                    input_color = torch.cat((input_color, torch.flip(input_color, [3])), 0)\n",
    "\n",
    "                output = depth_decoder(encoder(input_color))\n",
    "\n",
    "                pred_disp, _ = disp_to_depth(output[(\"disp\", 0)], opt.min_depth, opt.max_depth)\n",
    "                pred_disp = pred_disp.cpu()[:, 0].numpy()\n",
    "\n",
    "                if opt.post_process:\n",
    "                    N = pred_disp.shape[0] // 2\n",
    "                    pred_disp = batch_post_process_disparity(pred_disp[:N], pred_disp[N:, :, ::-1])\n",
    "\n",
    "                pred_disps.append(pred_disp)\n",
    "\n",
    "        pred_disps = np.concatenate(pred_disps)\n",
    "\n",
    "    else:\n",
    "        # Load predictions from file\n",
    "        print(\"-> Loading predictions from {}\".format(opt.ext_disp_to_eval))\n",
    "        pred_disps = np.load(opt.ext_disp_to_eval)\n",
    "\n",
    "        if opt.eval_eigen_to_benchmark:\n",
    "            eigen_to_benchmark_ids = np.load(\n",
    "                os.path.join(splits_dir, \"benchmark\", \"eigen_to_benchmark_ids.npy\"))\n",
    "\n",
    "            pred_disps = pred_disps[eigen_to_benchmark_ids]\n",
    "\n",
    "    if opt.save_pred_disps:\n",
    "        output_path = os.path.join(\n",
    "            opt.load_weights_folder, \"disps_{}_split.npy\".format(opt.eval_split))\n",
    "        print(\"-> Saving predicted disparities to \", output_path)\n",
    "        np.save(output_path, pred_disps)\n",
    "\n",
    "    if opt.no_eval:\n",
    "        print(\"-> Evaluation disabled. Done.\")\n",
    "        quit()\n",
    "\n",
    "    elif opt.eval_split == 'benchmark':\n",
    "        save_dir = os.path.join(opt.load_weights_folder, \"benchmark_predictions\")\n",
    "        print(\"-> Saving out benchmark predictions to {}\".format(save_dir))\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "\n",
    "        for idx in range(len(pred_disps)):\n",
    "            disp_resized = cv2.resize(pred_disps[idx], (1216, 352))\n",
    "            depth = STEREO_SCALE_FACTOR / disp_resized\n",
    "            depth = np.clip(depth, 0, 80)\n",
    "            depth = np.uint16(depth * 256)\n",
    "            save_path = os.path.join(save_dir, \"{:010d}.png\".format(idx))\n",
    "            cv2.imwrite(save_path, depth)\n",
    "\n",
    "        print(\"-> No ground truth is available for the KITTI benchmark, so not evaluating. Done.\")\n",
    "        quit()\n",
    "\n",
    "    gt_path = os.path.join(splits_dir, opt.eval_split, \"gt_depths.npz\")\n",
    "    gt_depths = np.load(gt_path, fix_imports=True, encoding='latin1')[\"data\"]\n",
    "\n",
    "    print(\"-> Evaluating\")\n",
    "\n",
    "    if opt.eval_stereo:\n",
    "        print(\"   Stereo evaluation - \"\n",
    "              \"disabling median scaling, scaling by {}\".format(STEREO_SCALE_FACTOR))\n",
    "        opt.disable_median_scaling = True\n",
    "        opt.pred_depth_scale_factor = STEREO_SCALE_FACTOR\n",
    "    else:\n",
    "        print(\"   Mono evaluation - using median scaling\")\n",
    "\n",
    "    errors = []\n",
    "    ratios = []\n",
    "\n",
    "    for i in range(pred_disps.shape[0]):\n",
    "\n",
    "        gt_depth = gt_depths[i]\n",
    "        gt_height, gt_width = gt_depth.shape[:2]\n",
    "\n",
    "        pred_disp = pred_disps[i]\n",
    "        pred_disp = cv2.resize(pred_disp, (gt_width, gt_height))\n",
    "        pred_depth = 1 / pred_disp\n",
    "\n",
    "        if opt.eval_split == \"eigen\":\n",
    "            mask = np.logical_and(gt_depth > MIN_DEPTH, gt_depth < MAX_DEPTH)\n",
    "\n",
    "            crop = np.array([0.40810811 * gt_height, 0.99189189 * gt_height,\n",
    "                             0.03594771 * gt_width,  0.96405229 * gt_width]).astype(np.int32)\n",
    "            crop_mask = np.zeros(mask.shape)\n",
    "            crop_mask[crop[0]:crop[1], crop[2]:crop[3]] = 1\n",
    "            mask = np.logical_and(mask, crop_mask)\n",
    "\n",
    "        else:\n",
    "            mask = gt_depth > 0\n",
    "\n",
    "        pred_depth = pred_depth[mask]\n",
    "        gt_depth = gt_depth[mask]\n",
    "\n",
    "        pred_depth *= opt.pred_depth_scale_factor\n",
    "        if not opt.disable_median_scaling:\n",
    "            ratio = np.median(gt_depth) / np.median(pred_depth)\n",
    "            ratios.append(ratio)\n",
    "            pred_depth *= ratio\n",
    "\n",
    "        pred_depth[pred_depth < MIN_DEPTH] = MIN_DEPTH\n",
    "        pred_depth[pred_depth > MAX_DEPTH] = MAX_DEPTH\n",
    "\n",
    "        errors.append(compute_errors(gt_depth, pred_depth))\n",
    "\n",
    "    if not opt.disable_median_scaling:\n",
    "        ratios = np.array(ratios)\n",
    "        med = np.median(ratios)\n",
    "        print(\" Scaling ratios | med: {:0.3f} | std: {:0.3f}\".format(med, np.std(ratios / med)))\n",
    "\n",
    "    mean_errors = np.array(errors).mean(0)\n",
    "\n",
    "    print(\"\\n  \" + (\"{:>8} | \" * 7).format(\"abs_rel\", \"sq_rel\", \"rmse\", \"rmse_log\", \"a1\", \"a2\", \"a3\"))\n",
    "    print((\"&{: 8.3f}  \" * 7).format(*mean_errors.tolist()) + \"\\\\\\\\\")\n",
    "    print(\"\\n-> Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: __main__.py [-h] [--data_path DATA_PATH] [--log_dir LOG_DIR]\n",
      "                   [--model_name MODEL_NAME]\n",
      "                   [--split {eigen_zhou,eigen_full,odom,benchmark}]\n",
      "                   [--num_layers {18,34,50,101,152}]\n",
      "                   [--dataset {kitti,kitti_odom,kitti_depth,kitti_test}]\n",
      "                   [--png] [--height HEIGHT] [--width WIDTH]\n",
      "                   [--disparity_smoothness DISPARITY_SMOOTHNESS]\n",
      "                   [--scales SCALES [SCALES ...]] [--min_depth MIN_DEPTH]\n",
      "                   [--max_depth MAX_DEPTH] [--use_stereo]\n",
      "                   [--frame_ids FRAME_IDS [FRAME_IDS ...]]\n",
      "                   [--batch_size BATCH_SIZE] [--learning_rate LEARNING_RATE]\n",
      "                   [--num_epochs NUM_EPOCHS]\n",
      "                   [--scheduler_step_size SCHEDULER_STEP_SIZE]\n",
      "                   [--v1_multiscale] [--avg_reprojection]\n",
      "                   [--disable_automasking] [--predictive_mask] [--no_ssim]\n",
      "                   [--weights_init {pretrained,scratch}]\n",
      "                   [--pose_model_input {pairs,all}]\n",
      "                   [--pose_model_type {posecnn,separate_resnet,shared}]\n",
      "                   [--no_cuda] [--num_workers NUM_WORKERS]\n",
      "                   [--load_weights_folder LOAD_WEIGHTS_FOLDER]\n",
      "                   [--models_to_load MODELS_TO_LOAD [MODELS_TO_LOAD ...]]\n",
      "                   [--log_frequency LOG_FREQUENCY]\n",
      "                   [--save_frequency SAVE_FREQUENCY] [--eval_stereo]\n",
      "                   [--eval_mono] [--disable_median_scaling]\n",
      "                   [--pred_depth_scale_factor PRED_DEPTH_SCALE_FACTOR]\n",
      "                   [--ext_disp_to_eval EXT_DISP_TO_EVAL]\n",
      "                   [--eval_split {eigen,eigen_benchmark,benchmark,odom_9,odom_10}]\n",
      "                   [--save_pred_disps] [--no_eval] [--eval_eigen_to_benchmark]\n",
      "                   [--eval_out_dir EVAL_OUT_DIR] [--post_process]\n",
      "__main__.py: error: unrecognized arguments: -f /run/user/1000/jupyter/kernel-ccde7330-8eb8-43bb-86fe-b2ac8b3b31d4.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2971: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "options = MonodepthOptions()\n",
    "evaluate(options.parse())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
